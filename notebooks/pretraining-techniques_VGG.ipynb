{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_meta = pd.read_csv('/Users/alex/Desktop/comet/data/training/X_meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Strawberry     7617\n",
       "Orange         5972\n",
       "Tomato         5826\n",
       "Apple          3703\n",
       "Grape          2588\n",
       "Lemon          1678\n",
       "Banana         1507\n",
       "Grapefruit     1185\n",
       "Watermelon      788\n",
       "Pear            741\n",
       "Peach           721\n",
       "Pomegranate     653\n",
       "Pineapple       630\n",
       "Mango           404\n",
       "Common fig      303\n",
       "Cantaloupe      158\n",
       "Name: LabelName, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_meta.LabelName.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempting to apply imbalanced learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are extremely inbalanced, with Cantaloupe having a fiftieth as many image samples in the dataset as Strawberry. If our goal is to build a classifer which is equally good for all of these image types, we'd benefit from rebalancing the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_meta[['CroppedImageURL']].values\n",
    "y = X_meta['LabelName'].values\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "X, y = RandomOverSampler(random_state=42).fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "split_ratio = 0.8\n",
    "n_samples = len(X)\n",
    "split_idx = int(split_ratio * n_samples)\n",
    "\n",
    "np.random.seed(42)\n",
    "idxs = np.arange(len(X))\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "X_train, X_test = X[idxs[:split_idx]], X[idxs[split_idx:]]\n",
    "y_train, y_test = y[idxs[:split_idx]], y[idxs[split_idx:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the `flow_from_dataframe` feature we need to have the latest version of `keras-preprocessing` installed. This library is not up-to-date in the `keras` version I have installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling Keras-Preprocessing-1.0.2:\n",
      "  Would remove:\n",
      "    /Users/alex/miniconda3/envs/open-fruits-dev/lib/python3.6/site-packages/Keras_Preprocessing-1.0.2-py2.7.egg-info\n",
      "    /Users/alex/miniconda3/envs/open-fruits-dev/lib/python3.6/site-packages/keras_preprocessing/*\n",
      "Proceed (y/n)? ^C\n",
      "\u001b[31mOperation cancelled by user\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/keras-team/keras-preprocessing.git\n",
      "  Cloning https://github.com/keras-team/keras-preprocessing.git to /private/var/folders/kx/vhz7qj2j2537dm7m86qllzx00000gn/T/pip-req-build-zvf8qnc0\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/alex/miniconda3/envs/open-fruits-dev/lib/python3.6/site-packages (from Keras-Preprocessing==1.0.9) (1.15.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/alex/miniconda3/envs/open-fruits-dev/lib/python3.6/site-packages (from Keras-Preprocessing==1.0.9) (1.12.0)\n",
      "Building wheels for collected packages: Keras-Preprocessing\n",
      "  Building wheel for Keras-Preprocessing (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /private/var/folders/kx/vhz7qj2j2537dm7m86qllzx00000gn/T/pip-ephem-wheel-cache-h40r8pft/wheels/03/a0/39/171f6040d36f36c71168dc69afa81334351b20955dc36ce932\n",
      "Successfully built Keras-Preprocessing\n",
      "\u001b[31mkeras 2.2.4 has requirement keras_applications>=1.0.6, but you'll have keras-applications 1.0.4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: Keras-Preprocessing\n",
      "  Found existing installation: Keras-Preprocessing 1.0.2\n",
      "    Uninstalling Keras-Preprocessing-1.0.2:\n",
      "      Successfully uninstalled Keras-Preprocessing-1.0.2\n",
      "Successfully installed Keras-Preprocessing-1.0.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U git+https://github.com/keras-team/keras-preprocessing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    rescale=1/255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1/255\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to coax `keras` into respecting oversampling I will use the `from_from_dataframe` facility targeting the oversampled `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 972 validated image filenames belonging to 16 classes.\n",
      "Found 13765 validated image filenames belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "X_train_df = pd.DataFrame().assign(ImagePath=X_train[:, 0], ImageClass=y_train)\n",
    "X_test_df = pd.DataFrame().assign(ImagePath=X_test[:, 0], ImageClass=y_test)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    X_train_df,\n",
    "    directory='/Users/alex/Desktop/comet/data/images_cropped/',\n",
    "    x_col='ImagePath',\n",
    "    y_col='ImageClass',\n",
    "    target_size=(48, 48),\n",
    "    batch_size=512,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_dataframe(\n",
    "    X_test_df,\n",
    "    directory='/Users/alex/Desktop/comet/data/images_cropped/',\n",
    "    x_col='ImagePath',\n",
    "    y_col='ImageClass',    \n",
    "    target_size=(48, 48),\n",
    "    batch_size=512,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97497\n",
      "30581\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_df))\n",
    "print(len(train_generator.filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that unfortunately the training generator does not respect oversampling in its input `DataFrame`. If we want to oversample and still use the tranining generators, it appears that we will have to write copies of image files directly to disk. We could omit the traning generator, but then we'd have to form a whole-dataset `numpy` array, which would take more RAM than this computer has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1&mdash;Bottleneck VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottleneck VGG comes from [\"Building powerful image classification models using very little data\"](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html).\n",
    "\n",
    "Bottlenecking is a simplistic but extremely fast pretraining technique. We begin by passing the dataset through a pretrained model with the top (fully connected) layer omitted. By running all samples through this model, we generate the sparsified, featurized representation that that model learned for the given piece of data. That sample is then fed as input to our simpler custom model, which trains on these inputs in order to generate model results.\n",
    "\n",
    "This is a fast pretraining technique because it is as fast as training the simple model we use on top of the bottleneck, plus passing all data through the pretrained model once.\n",
    "\n",
    "**Note**: to verify that this code was correct, I did `head(1000)` on the input `DataFrame`. This made it so I could complete one round of training locally, just to make sure that all of the sizes align."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "model = keras.applications.VGG16(include_top=False, weights='imagenet')\n",
    "bottleneck_features_train = model.predict_generator(\n",
    "    train_generator,\n",
    "    steps=len(train_generator.filenames) // batch_size\n",
    ")\n",
    "\n",
    "bottleneck_features_test = model.predict_generator(\n",
    "    test_generator, \n",
    "    steps=len(train_generator.filenames) // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the last layer is a list of convolutional feature weights (as returned by a final `MaxPool` layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3718927 , 0.        , 0.        , 0.7131844 , 0.        ,\n",
       "       0.        , 0.        , 0.54641426, 0.        , 0.7506968 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.4727954 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.6142101 , 0.        ,\n",
       "       0.        , 1.4543753 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.58099055, 0.5119456 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.3331678 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.84954274,\n",
       "       0.25664645, 0.        , 0.08509421, 0.        , 1.829702  ,\n",
       "       0.        , 0.        , 0.2652532 , 0.505351  , 0.        ,\n",
       "       0.        , 0.464088  , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.2156435 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.1929902 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.22787061, 0.        , 0.9586344 , 0.        , 0.03910017,\n",
       "       0.        , 0.        , 0.9565393 , 0.        , 1.1658722 ,\n",
       "       0.63576543, 0.        , 0.02215932, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.66392946,\n",
       "       0.        , 0.        , 0.        , 0.3043135 , 2.3565125 ,\n",
       "       0.        , 0.        , 0.        , 0.89487004, 0.        ,\n",
       "       0.        , 0.08743162, 1.5844958 , 0.        , 0.        ,\n",
       "       0.2593022 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.0378126 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.11533055,\n",
       "       0.        , 0.3418855 , 0.        , 0.19474879, 0.        ,\n",
       "       0.        , 0.        , 1.7421823 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.49975303, 0.        , 0.        , 0.        , 0.08867388,\n",
       "       9.369841  , 0.        , 0.7975006 , 0.        , 0.6244586 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.36669552, 0.13852853, 0.29263002,\n",
       "       0.02922636, 0.        , 0.37429318, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18000682, 0.        ,\n",
       "       0.62255454, 0.        , 1.0925162 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.07538534,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.4512852 , 0.09138579,\n",
       "       0.        , 0.        , 0.        , 0.57582176, 0.        ,\n",
       "       0.        , 0.        , 0.11605925, 0.26858228, 0.        ,\n",
       "       0.        , 0.        , 0.3376674 , 0.        , 0.        ,\n",
       "       0.        , 0.24235938, 0.        , 0.        , 0.        ,\n",
       "       0.22585987, 0.08961153, 0.        , 0.        , 0.        ,\n",
       "       0.5507269 , 0.        , 0.        , 0.        , 0.40381   ,\n",
       "       1.2676772 , 0.        , 1.4174732 , 1.447612  , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.9060784 ,\n",
       "       0.        , 0.        , 0.95404637, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07459816, 0.        , 0.        ,\n",
       "       0.4969279 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.14712724, 0.        , 0.        , 0.        ,\n",
       "       0.6783251 , 0.        , 0.        , 0.        , 0.5037517 ,\n",
       "       0.37045532, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.18833882, 1.475771  , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10589987, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.5008142 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.094738  , 0.7260705 , 0.6923154 , 0.        , 0.        ,\n",
       "       0.9582948 , 0.        , 0.99752724, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.742578  ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.25335312, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.7172229 , 0.        , 0.        , 0.17120281, 0.        ,\n",
       "       0.        , 0.928594  , 0.        , 0.        , 1.2016927 ,\n",
       "       0.        , 0.        , 0.87627494, 0.14644846, 0.17312561,\n",
       "       0.        , 0.4169815 , 1.430702  , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.233118  , 0.67936397, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.3990334 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.8631396 , 0.        , 0.        , 0.6195783 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.73247933,\n",
       "       0.4903727 , 0.        , 0.        , 0.62847096, 0.        ,\n",
       "       1.0907989 , 0.        , 0.        , 0.15747744, 0.        ,\n",
       "       0.7585319 , 0.73931605, 0.        , 1.4487882 , 0.        ,\n",
       "       0.359257  , 0.2659787 , 0.        , 0.        , 0.        ,\n",
       "       0.03757679, 0.        , 0.33971086, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.06292918, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.7521537 , 0.        , 0.        , 0.89477676,\n",
       "       0.        , 2.4089782 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.49635047, 0.        , 0.1123849 ,\n",
       "       0.27353358, 0.        , 0.        , 0.        , 1.1471778 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.13625538, 0.56953067, 0.        ,\n",
       "       0.04461943, 0.7889475 , 0.03397869, 0.        , 0.08759744,\n",
       "       0.        , 0.        , 0.37978333, 0.        , 1.883595  ,\n",
       "       0.09111173, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.1916253 , 0.11069646, 0.        , 0.        ,\n",
       "       0.        , 0.2530182 , 0.12696278, 0.        , 0.        ,\n",
       "       0.        , 0.24987635, 0.        , 0.        , 1.7668674 ,\n",
       "       0.5277702 , 0.05375838, 0.        , 0.15176022, 0.        ,\n",
       "       0.563791  , 0.04623914, 0.93172055, 0.11066052, 0.        ,\n",
       "       0.        , 0.04789963, 0.2303554 , 0.        , 0.        ,\n",
       "       0.03996313, 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck_features_train[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.pooling.MaxPooling2D at 0x1a269fd2b0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shape is interesting and requires flattening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(972, 1, 1, 512)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck_features_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the blog post saves these to a `numpy` array and reads them back out, the best way to do this is actually to put the new layers that you want directly on top of the existing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "input_shape = (1, 1, 512)  # == bottleneck_features_train.shape[1:]\n",
    "\n",
    "prior = keras.applications.VGG16(\n",
    "    include_top=False, \n",
    "    weights='imagenet',\n",
    "    input_shape=(48, 48, 3)\n",
    ")\n",
    "model = Sequential()\n",
    "model.add(prior)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', name='Dense_Intermediate'))\n",
    "model.add(Dropout(0.2, name='Dropout_Regularization'))\n",
    "model.add(Dense(16, activation='sigmoid', name='Output'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: you must specify the `input_shape` on the `keras.applications.VGG16` layer. if you do not specify it the model will assume a just-in-time `(None, None, None, 512)` shape (where the first `None` is the number of samples and the other two are god knows what). `Flatten` cannot work with this input shape. It complains that it doesn't have enough information to do its job.\n",
    "\n",
    "**Note**: `48x48` is the smallest legal input size for `VGG16`. Keras will literally not allow you to specify inputs smaller than that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 62s 62s/step - loss: 2.8166 - acc: 0.0348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a64985438>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator.filenames) // batch_size,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "\n",
    "X_meta = pd.read_csv('/Users/alex/Desktop/comet/data/training/X_meta.csv')\n",
    "X = X_meta[['CroppedImageURL']].values\n",
    "y = X_meta['LabelName'].values\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "idxs = np.arange(len(X))\n",
    "np.random.shuffle(idxs)\n",
    "X_train, X_test = X[idxs[:split_idx]], X[idxs[split_idx:]]\n",
    "y_train, y_test = y[idxs[:split_idx]], y[idxs[split_idx:]]\n",
    "X_train_df = pd.DataFrame().assign(ImagePath=X_train[:, 0], ImageClass=y_train)\n",
    "X_test_df = pd.DataFrame().assign(ImagePath=X_test[:, 0], ImageClass=y_test)\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    rescale=1/255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1/255\n",
    ")\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    X_train_df,\n",
    "    directory='/Users/alex/Desktop/comet/data/images_cropped/',\n",
    "    x_col='ImagePath',\n",
    "    y_col='ImageClass',\n",
    "    target_size=(48, 48),\n",
    "    batch_size=512,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "validation_generator = test_datagen.flow_from_dataframe(\n",
    "    X_test_df,\n",
    "    directory='/Users/alex/Desktop/comet/data/images_cropped/',\n",
    "    x_col='ImagePath',\n",
    "    y_col='ImageClass',\n",
    "    target_size=(48, 48),\n",
    "    batch_size=512,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "model = keras.applications.VGG16(include_top=False, weights='imagenet')\n",
    "prior = keras.applications.VGG16(\n",
    "    include_top=False, \n",
    "    weights='imagenet',\n",
    "    input_shape=(48, 48, 3)\n",
    ")\n",
    "model = Sequential()\n",
    "model.add(prior)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', name='Dense_Intermediate'))\n",
    "model.add(Dropout(0.2, name='Dropout_Regularization'))\n",
    "model.add(Dense(16, activation='sigmoid', name='Output'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator.filenames) // batch_size,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modified version of this script, fitted to use `comet` and `t4` is `models/bottleneck_model.py`.\n",
    "\n",
    "**Note**: the original run generating that model was done via `alekseylearn` on AWS SageMaker, hence why it ends by running `model.save('/opt/ml/model/model.h5')`. I downloaded the artifact to `bottleneck_model.h5` locally before pushing it to remote storage as a package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Hashing', max=59998064, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Copying', max=59998064, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(remote Package)\n",
       " └─bottleneck_model.h5"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import t4\n",
    "t4.Package().set('bottleneck_model.h5', 'bottleneck_model.h5').push('quilt/open_fruit_models', 's3://quilt-example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2&mdash;Pretrained VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bottleneck approach is fast, but we don't do any training on the pretrained model. If we did some training there too, we'd improve our score even further.\n",
    "\n",
    "We will unfreeze the topmost convolutional block of the network (this has the least generalized features, and will get us the most gain from retraining), then train again. To do this sucessfully we need to do a few things:\n",
    "\n",
    "1. Use a pre-trained model for the top layer, as otherwise the weight updates will wreck the existing features in the unfrozen CNN block.\n",
    "2. Set a small non-adaptive learning rate, e.g. a small SGD value. Again for the same reason.\n",
    "\n",
    "We will reuse the topmost layer of the bottleneck model's output for this purpose. If you were starting from scratch you could pretrain this simple neural network using an [autoencoder](https://www.kaggle.com/residentmario/autoencoders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26425c5c4cde4e63aef40feb3546d7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Copying', max=59998064, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/miniconda3/envs/open-fruits-dev/lib/python3.6/site-packages/keras/engine/saving.py:327: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "# fetch the pretrained model from storage\n",
    "import t4\n",
    "t4.Package.browse('quilt/open_fruit_models', 's3://quilt-example')['bottleneck_model.h5']\\\n",
    "    .fetch('bottleneck_model.h5')\n",
    "pretrained_model = keras.models.load_model('bottleneck_model.h5')\n",
    "\n",
    "\n",
    "# define the model\n",
    "input_shape = (1, 1, 512)  # == bottleneck_features_train.shape[1:]\n",
    "prior = keras.applications.VGG16(\n",
    "    include_top=False, \n",
    "    weights='imagenet',\n",
    "    input_shape=(48, 48, 3)\n",
    ")\n",
    "model = Sequential()\n",
    "model.add(prior)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', name='Dense_Intermediate'))\n",
    "model.add(Dropout(0.2, name='Dropout_Regularization'))\n",
    "model.add(Dense(16, activation='sigmoid', name='Output'))\n",
    "\n",
    "\n",
    "# set pretrained weights\n",
    "for new_layer, old_layer in zip(model.layers[-4:], pretrained_model.layers[-4:]):\n",
    "    new_layer.set_weights(old_layer.get_weights())\n",
    "\n",
    "\n",
    "# leave the outermost convblock trainable, but freeze all other layers\n",
    "for cnn_block_layer in model.layers[0].layers[:-4]:\n",
    "    cnn_block_layer.trainable = False\n",
    "\n",
    "    \n",
    "# compile the model\n",
    "model.compile(\n",
    "    # one-tenth the standard SGD learning rate w/ some momentum\n",
    "    optimizer=SGD(lr=1e-4, momentum=0.9),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to test that the model compiles successfully\n",
    "# model.fit_generator(\n",
    "#     train_generator,\n",
    "#     # use a tiny batch size just to verify that the model trains correctly\n",
    "#     steps_per_epoch=len(train_generator.filenames) // 8,\n",
    "#     epochs=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3&mdash;Pretrained VGG16 with Progressive Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.applications.VGG16??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
