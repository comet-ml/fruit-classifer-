{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Localize the quilt data for the fruit dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment, ExistingExperiment\n",
    "import t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4.Package.install(\n",
    "    \"quilt/open_fruit\", \n",
    "    registry=\"s3://quilt-example\", \n",
    "    dest=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ceceshao1/comet-quilt-example/d0625f21ccb34e83b9a338092016cf83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(project_name=\"comet-quilt-example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dropout, Flatten, Dense  \n",
    "from keras import applications  \n",
    "from keras.utils.np_utils import to_categorical  \n",
    "import matplotlib.pyplot as plt  \n",
    "import math  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images\n",
    "img_width, img_height = 128, 128\n",
    "\n",
    "# set parameters\n",
    "batch_size = 16\n",
    "num_classes = 16\n",
    "epochs = 50\n",
    "activation = 'relu'\n",
    "lr = 0.01\n",
    "min_delta=0\n",
    "patience=4\n",
    "dropout=0.2\n",
    "\n",
    "train_samples = 27593\n",
    "validation_samples = 6889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'batch_size':batch_size,\n",
    "        'epochs':epochs,\n",
    "        'min_delta':min_delta,\n",
    "        'patience':patience,\n",
    "        'learning_rate':lr,\n",
    "        'dropout':dropout\n",
    "}\n",
    "\n",
    "experiment.log_parameters(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing + Augmentation\n",
    "\n",
    "Since our quilt data package does not have pre-defined training and validation subdirectories, we can use the `validation_split` argument for the ImageDataGenerator(). Here we split the data into 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27593 images belonging to 16 classes.\n",
      "Found 6889 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    rescale=1/255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2 #set validation split\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1/255\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    './data/quilt/open_fruit/images_cropped',\n",
    "    target_size=(128, 128),\n",
    "    shuffle=True,\n",
    "    seed=20,\n",
    "    batch_size = batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    './data/quilt/open_fruit/images_cropped',\n",
    "    target_size=(128, 128),\n",
    "    seed=20,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset = \"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Apple': 0,\n",
       " 'Banana': 1,\n",
       " 'Cantaloupe': 2,\n",
       " 'Common_fig': 3,\n",
       " 'Grape': 4,\n",
       " 'Grapefruit': 5,\n",
       " 'Lemon': 6,\n",
       " 'Mango': 7,\n",
       " 'Orange': 8,\n",
       " 'Peach': 9,\n",
       " 'Pear': 10,\n",
       " 'Pineapple': 11,\n",
       " 'Pomegranate': 12,\n",
       " 'Strawberry': 13,\n",
       " 'Tomato': 14,\n",
       " 'Watermelon': 15}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the classes and their index\n",
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Architecture - Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(128, 128, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(units=16, activation='softmax')) #Output Layer - Activation Function Softmax(to clasify multiple classes)\n",
    "\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=RMSprop(lr=lr),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "sample_size = len(list(pathlib.Path('./data/quilt/open_fruit/images_cropped').rglob('./*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34498"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2156/2156 [==============================] - 484s 225ms/step - loss: 12.5476 - acc: 0.2211 - val_loss: 12.5524 - val_acc: 0.2212\n",
      "Epoch 2/50\n",
      "2156/2156 [==============================] - 480s 223ms/step - loss: 12.6031 - acc: 0.2181 - val_loss: 12.5524 - val_acc: 0.2212\n",
      "Epoch 3/50\n",
      "2156/2156 [==============================] - 478s 222ms/step - loss: 12.5244 - acc: 0.2230 - val_loss: 12.5524 - val_acc: 0.2212\n",
      "Epoch 4/50\n",
      "2156/2156 [==============================] - 475s 220ms/step - loss: 12.5542 - acc: 0.2211 - val_loss: 12.5524 - val_acc: 0.2212\n",
      "Epoch 5/50\n",
      "2156/2156 [==============================] - 479s 222ms/step - loss: 12.5913 - acc: 0.2188 - val_loss: 12.5524 - val_acc: 0.2212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd9ee269748>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=sample_size // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_samples // batch_size,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', min_delta=min_delta, patience=patience)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Metrics:\n",
      "COMET INFO:                           acc: 0.21885238771782306\n",
      "COMET INFO:                         batch: 2150\n",
      "COMET INFO:                     epoch_end: 4\n",
      "COMET INFO:                          loss: 12.590611759420165\n",
      "COMET INFO:                          size: 16\n",
      "COMET INFO:                          step: 10780\n",
      "COMET INFO:         sys.gpu.0.free_memory: 150929408\n",
      "COMET INFO:     sys.gpu.0.gpu_utilization: 0\n",
      "COMET INFO:        sys.gpu.0.total_memory: 11996954624\n",
      "COMET INFO:         sys.gpu.0.used_memory: 11846025216\n",
      "COMET INFO:                       val_acc: 0.22122093023255815\n",
      "COMET INFO:                      val_loss: 12.552435361507326\n",
      "COMET INFO:   Other:\n",
      "COMET INFO:     trainable_params: 832560\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Uploading stats to Comet before program termination (may take several seconds)\n"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('simple_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.log_asset(file_path='/simple_cnn.h5', file_name='simple_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
